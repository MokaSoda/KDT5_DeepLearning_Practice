{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn               # LinearRegression 기능의 클래스 Linear\n",
    "import torch.optim as optim         # 최적화 (경사하강법)\n",
    "import torch.nn.functional as F     # 손실함수 로딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_name = 'Fashion-MNIST'\n",
    "X, y = fetch_openml(name=db_name, parser='auto', as_frame=False, return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_train = torch.from_numpy(X[:60000]).type(torch.float32)\n",
    "yt_train = torch.from_numpy(y[:60000].astype(np.uint8)).type(torch.long)\n",
    "Xt_test = torch.from_numpy(X[60000:]).type(torch.float32)\n",
    "yt_test = torch.from_numpy(y[60000:].astype(np.uint8)).type(torch.long)\n",
    "\n",
    "# assign to cuda\n",
    "Xt_train, yt_train = Xt_train.to(device), yt_train.to(device)\n",
    "Xt_test, yt_test = Xt_test.to(device), yt_test.to(device)\n",
    "\n",
    "yt_train = F.one_hot(yt_train, num_classes=10).type(torch.float32)\n",
    "yt_test = F.one_hot(yt_test, num_classes=10).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    \"\"\"다중 분류 모델, \n",
    "    활성함수는 별도의 loss function \n",
    "    에 내장된 cross_entropy를 사용\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(model, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(784, 256, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10, device=device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cross_entropy(y_pred, y_true, y_test_pred, y_test_true):\n",
    "    trainscore =  torch.mean((torch.argmax(y_pred, dim=1) == torch.argmax(y_true, dim=1)).type(torch.float32))\n",
    "    valscore =  torch.mean((torch.argmax(y_test_pred, dim=1) == torch.argmax(y_test_true, dim=1)).type(torch.float32))\n",
    "    return trainscore, valscore\n",
    "\n",
    "\n",
    "def train(model, X_train, y_train, X_test, y_test, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    totalresult = [[],[]]\n",
    "    optimizer = optimizer(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=5, verbose=True, eps=1e-8)\n",
    "    # try:\n",
    "    #     if not torch.cuda.is_available():\n",
    "    #         import intel_extension_for_pytorch as ipex\n",
    "    #         model, optimizer = ipex.optimize(model, optimizer = optimizer)\n",
    "    # except:\n",
    "    #     print(\"Intel Ipex Disabled\")\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "        y_pred = model(X_train)\n",
    "        y_test_pred  = model(X_test)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        val_loss = criterion(y_test_pred, y_test)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        scheduler.step(val_loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        result = accuracy_cross_entropy(y_pred, y_train, y_test_pred, y_test)\n",
    "        totalresult[0].append(result[0])\n",
    "        totalresult[1].append(result[1])\n",
    "        \n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()},\\n'\n",
    "              f'Acurracy : (1) train {result[0]:.3f}, (2) test {result[1]:.3f}')\n",
    "        \n",
    "        if scheduler.num_bad_epochs > scheduler.patience:\n",
    "            print(f'Early stopping at epoch {epoch} ... ')\n",
    "            break\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workplace/miniconda3/envs/EXAM_DL2/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 18.63163185119629,\n",
      "Acurracy : (1) train 0.129, (2) test 0.128\n",
      "Epoch 1, Loss: 22.389232635498047,\n",
      "Acurracy : (1) train 0.178, (2) test 0.184\n",
      "Epoch 2, Loss: 32.15156555175781,\n",
      "Acurracy : (1) train 0.206, (2) test 0.211\n",
      "Epoch 3, Loss: 16.61124610900879,\n",
      "Acurracy : (1) train 0.217, (2) test 0.220\n",
      "Epoch 4, Loss: 12.173739433288574,\n",
      "Acurracy : (1) train 0.259, (2) test 0.260\n",
      "Epoch 5, Loss: 8.669458389282227,\n",
      "Acurracy : (1) train 0.368, (2) test 0.365\n",
      "Epoch 6, Loss: 6.756321430206299,\n",
      "Acurracy : (1) train 0.428, (2) test 0.422\n",
      "Epoch 7, Loss: 4.763392925262451,\n",
      "Acurracy : (1) train 0.434, (2) test 0.432\n",
      "Epoch 8, Loss: 3.3811144828796387,\n",
      "Acurracy : (1) train 0.505, (2) test 0.498\n",
      "Epoch 9, Loss: 2.8291521072387695,\n",
      "Acurracy : (1) train 0.525, (2) test 0.521\n",
      "Epoch 10, Loss: 2.6364264488220215,\n",
      "Acurracy : (1) train 0.537, (2) test 0.536\n",
      "Epoch 11, Loss: 2.381291151046753,\n",
      "Acurracy : (1) train 0.541, (2) test 0.538\n",
      "Epoch 12, Loss: 1.534411072731018,\n",
      "Acurracy : (1) train 0.592, (2) test 0.591\n",
      "Epoch 13, Loss: 1.8066890239715576,\n",
      "Acurracy : (1) train 0.563, (2) test 0.554\n",
      "Epoch 14, Loss: 1.683670163154602,\n",
      "Acurracy : (1) train 0.567, (2) test 0.563\n",
      "Epoch 15, Loss: 1.367537260055542,\n",
      "Acurracy : (1) train 0.625, (2) test 0.618\n",
      "Epoch 16, Loss: 1.3891420364379883,\n",
      "Acurracy : (1) train 0.667, (2) test 0.661\n",
      "Epoch 17, Loss: 1.2880799770355225,\n",
      "Acurracy : (1) train 0.673, (2) test 0.664\n",
      "Epoch 18, Loss: 1.1382091045379639,\n",
      "Acurracy : (1) train 0.672, (2) test 0.663\n",
      "Epoch 19, Loss: 1.1525315046310425,\n",
      "Acurracy : (1) train 0.645, (2) test 0.641\n",
      "Epoch 20, Loss: 1.0979524850845337,\n",
      "Acurracy : (1) train 0.651, (2) test 0.643\n",
      "Epoch 21, Loss: 0.9657639265060425,\n",
      "Acurracy : (1) train 0.691, (2) test 0.684\n",
      "Epoch 22, Loss: 0.9240960478782654,\n",
      "Acurracy : (1) train 0.711, (2) test 0.701\n",
      "Epoch 23, Loss: 0.9045071601867676,\n",
      "Acurracy : (1) train 0.722, (2) test 0.720\n",
      "Epoch 24, Loss: 0.8985751271247864,\n",
      "Acurracy : (1) train 0.719, (2) test 0.713\n",
      "Epoch 25, Loss: 0.8698161840438843,\n",
      "Acurracy : (1) train 0.726, (2) test 0.714\n",
      "Epoch 26, Loss: 0.8548910617828369,\n",
      "Acurracy : (1) train 0.719, (2) test 0.709\n",
      "Epoch 27, Loss: 0.8014283180236816,\n",
      "Acurracy : (1) train 0.733, (2) test 0.722\n",
      "Epoch 28, Loss: 0.7671728134155273,\n",
      "Acurracy : (1) train 0.749, (2) test 0.738\n",
      "Epoch 29, Loss: 0.778496503829956,\n",
      "Acurracy : (1) train 0.746, (2) test 0.736\n",
      "Epoch 30, Loss: 0.754604160785675,\n",
      "Acurracy : (1) train 0.753, (2) test 0.744\n",
      "Epoch 31, Loss: 0.7338953614234924,\n",
      "Acurracy : (1) train 0.757, (2) test 0.748\n",
      "Epoch 32, Loss: 0.7019121050834656,\n",
      "Acurracy : (1) train 0.766, (2) test 0.759\n",
      "Epoch 33, Loss: 0.6978476643562317,\n",
      "Acurracy : (1) train 0.769, (2) test 0.762\n",
      "Epoch 34, Loss: 0.6924816966056824,\n",
      "Acurracy : (1) train 0.770, (2) test 0.762\n",
      "Epoch 35, Loss: 0.6720766425132751,\n",
      "Acurracy : (1) train 0.773, (2) test 0.764\n",
      "Epoch 36, Loss: 0.6560778617858887,\n",
      "Acurracy : (1) train 0.776, (2) test 0.764\n",
      "Epoch 37, Loss: 0.6461200714111328,\n",
      "Acurracy : (1) train 0.779, (2) test 0.768\n",
      "Epoch 38, Loss: 0.6411174535751343,\n",
      "Acurracy : (1) train 0.779, (2) test 0.769\n",
      "Epoch 39, Loss: 0.6293432116508484,\n",
      "Acurracy : (1) train 0.782, (2) test 0.772\n",
      "Epoch 40, Loss: 0.6145504117012024,\n",
      "Acurracy : (1) train 0.786, (2) test 0.777\n",
      "Epoch 41, Loss: 0.6101024746894836,\n",
      "Acurracy : (1) train 0.786, (2) test 0.776\n",
      "Epoch 42, Loss: 0.607215940952301,\n",
      "Acurracy : (1) train 0.788, (2) test 0.781\n",
      "Epoch 43, Loss: 0.5984914898872375,\n",
      "Acurracy : (1) train 0.791, (2) test 0.783\n",
      "Epoch 44, Loss: 0.5856547355651855,\n",
      "Acurracy : (1) train 0.793, (2) test 0.786\n",
      "Epoch 45, Loss: 0.5803012847900391,\n",
      "Acurracy : (1) train 0.794, (2) test 0.787\n",
      "Epoch 46, Loss: 0.5784938335418701,\n",
      "Acurracy : (1) train 0.795, (2) test 0.789\n",
      "Epoch 47, Loss: 0.5731233954429626,\n",
      "Acurracy : (1) train 0.796, (2) test 0.790\n",
      "Epoch 48, Loss: 0.5664607882499695,\n",
      "Acurracy : (1) train 0.797, (2) test 0.792\n",
      "Epoch 49, Loss: 0.5593773722648621,\n",
      "Acurracy : (1) train 0.800, (2) test 0.793\n",
      "Epoch 50, Loss: 0.5565351843833923,\n",
      "Acurracy : (1) train 0.801, (2) test 0.793\n",
      "Epoch 51, Loss: 0.5527980327606201,\n",
      "Acurracy : (1) train 0.803, (2) test 0.795\n",
      "Epoch 52, Loss: 0.549268901348114,\n",
      "Acurracy : (1) train 0.805, (2) test 0.799\n",
      "Epoch 53, Loss: 0.5448268055915833,\n",
      "Acurracy : (1) train 0.807, (2) test 0.799\n",
      "Epoch 54, Loss: 0.5401996374130249,\n",
      "Acurracy : (1) train 0.808, (2) test 0.800\n",
      "Epoch 55, Loss: 0.5369899868965149,\n",
      "Acurracy : (1) train 0.809, (2) test 0.799\n",
      "Epoch 56, Loss: 0.5334998965263367,\n",
      "Acurracy : (1) train 0.810, (2) test 0.801\n",
      "Epoch 57, Loss: 0.5306761860847473,\n",
      "Acurracy : (1) train 0.811, (2) test 0.802\n",
      "Epoch 58, Loss: 0.5267986059188843,\n",
      "Acurracy : (1) train 0.812, (2) test 0.801\n",
      "Epoch 59, Loss: 0.5235399603843689,\n",
      "Acurracy : (1) train 0.814, (2) test 0.804\n",
      "Epoch 60, Loss: 0.5203683972358704,\n",
      "Acurracy : (1) train 0.815, (2) test 0.807\n",
      "Epoch 61, Loss: 0.5173384547233582,\n",
      "Acurracy : (1) train 0.817, (2) test 0.808\n",
      "Epoch 62, Loss: 0.5144217014312744,\n",
      "Acurracy : (1) train 0.818, (2) test 0.810\n",
      "Epoch 63, Loss: 0.5111784338951111,\n",
      "Acurracy : (1) train 0.818, (2) test 0.810\n",
      "Epoch 64, Loss: 0.5084308981895447,\n",
      "Acurracy : (1) train 0.818, (2) test 0.810\n",
      "Epoch 65, Loss: 0.5054334402084351,\n",
      "Acurracy : (1) train 0.820, (2) test 0.811\n",
      "Epoch 66, Loss: 0.5028112530708313,\n",
      "Acurracy : (1) train 0.820, (2) test 0.812\n",
      "Epoch 67, Loss: 0.5000783801078796,\n",
      "Acurracy : (1) train 0.821, (2) test 0.812\n",
      "Epoch 68, Loss: 0.4977012276649475,\n",
      "Acurracy : (1) train 0.822, (2) test 0.812\n",
      "Epoch 69, Loss: 0.4951109290122986,\n",
      "Acurracy : (1) train 0.823, (2) test 0.815\n",
      "Epoch 70, Loss: 0.4923211336135864,\n",
      "Acurracy : (1) train 0.825, (2) test 0.815\n",
      "Epoch 71, Loss: 0.48981940746307373,\n",
      "Acurracy : (1) train 0.825, (2) test 0.817\n",
      "Epoch 72, Loss: 0.4876106083393097,\n",
      "Acurracy : (1) train 0.825, (2) test 0.817\n",
      "Epoch 73, Loss: 0.48531627655029297,\n",
      "Acurracy : (1) train 0.826, (2) test 0.818\n",
      "Epoch 74, Loss: 0.4825827479362488,\n",
      "Acurracy : (1) train 0.828, (2) test 0.818\n",
      "Epoch 75, Loss: 0.48036420345306396,\n",
      "Acurracy : (1) train 0.828, (2) test 0.819\n",
      "Epoch 76, Loss: 0.4783960282802582,\n",
      "Acurracy : (1) train 0.829, (2) test 0.820\n",
      "Epoch 77, Loss: 0.47644585371017456,\n",
      "Acurracy : (1) train 0.830, (2) test 0.820\n",
      "Epoch 78, Loss: 0.4741559326648712,\n",
      "Acurracy : (1) train 0.831, (2) test 0.822\n",
      "Epoch 79, Loss: 0.4721021056175232,\n",
      "Acurracy : (1) train 0.832, (2) test 0.822\n",
      "Epoch 80, Loss: 0.47022876143455505,\n",
      "Acurracy : (1) train 0.832, (2) test 0.823\n",
      "Epoch 81, Loss: 0.46838292479515076,\n",
      "Acurracy : (1) train 0.833, (2) test 0.822\n",
      "Epoch 82, Loss: 0.46641069650650024,\n",
      "Acurracy : (1) train 0.834, (2) test 0.823\n",
      "Epoch 83, Loss: 0.464508593082428,\n",
      "Acurracy : (1) train 0.834, (2) test 0.825\n",
      "Epoch 84, Loss: 0.4626324474811554,\n",
      "Acurracy : (1) train 0.835, (2) test 0.825\n",
      "Epoch 85, Loss: 0.46080461144447327,\n",
      "Acurracy : (1) train 0.835, (2) test 0.826\n",
      "Epoch 86, Loss: 0.45899882912635803,\n",
      "Acurracy : (1) train 0.836, (2) test 0.826\n",
      "Epoch 87, Loss: 0.4572276175022125,\n",
      "Acurracy : (1) train 0.837, (2) test 0.826\n",
      "Epoch 88, Loss: 0.45549026131629944,\n",
      "Acurracy : (1) train 0.838, (2) test 0.827\n",
      "Epoch 89, Loss: 0.4537544548511505,\n",
      "Acurracy : (1) train 0.838, (2) test 0.827\n",
      "Epoch 90, Loss: 0.45210567116737366,\n",
      "Acurracy : (1) train 0.838, (2) test 0.827\n",
      "Epoch 91, Loss: 0.4504793882369995,\n",
      "Acurracy : (1) train 0.839, (2) test 0.828\n",
      "Epoch 92, Loss: 0.44886720180511475,\n",
      "Acurracy : (1) train 0.840, (2) test 0.828\n",
      "Epoch 93, Loss: 0.447273850440979,\n",
      "Acurracy : (1) train 0.840, (2) test 0.828\n",
      "Epoch 94, Loss: 0.4457426965236664,\n",
      "Acurracy : (1) train 0.841, (2) test 0.828\n",
      "Epoch 95, Loss: 0.4442083239555359,\n",
      "Acurracy : (1) train 0.841, (2) test 0.829\n",
      "Epoch 96, Loss: 0.4426695704460144,\n",
      "Acurracy : (1) train 0.842, (2) test 0.829\n",
      "Epoch 97, Loss: 0.4411325752735138,\n",
      "Acurracy : (1) train 0.842, (2) test 0.831\n",
      "Epoch 98, Loss: 0.4396618604660034,\n",
      "Acurracy : (1) train 0.842, (2) test 0.831\n",
      "Epoch 99, Loss: 0.43819648027420044,\n",
      "Acurracy : (1) train 0.843, (2) test 0.832\n",
      "Epoch 100, Loss: 0.43672510981559753,\n",
      "Acurracy : (1) train 0.844, (2) test 0.832\n"
     ]
    }
   ],
   "source": [
    "cloth = model(device)\n",
    "criteria = F.cross_entropy\n",
    "optimizer = optim.Adam\n",
    "epochs = 100\n",
    "train(\n",
    "    cloth, \n",
    "    Xt_train, \n",
    "    yt_train, \n",
    "    Xt_test, \n",
    "    yt_test, \n",
    "    criteria, \n",
    "    optimizer, \n",
    "    epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for x in cloth.parameters():\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
