{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset 클래스\n",
    "- 다음과 같이 데이터 로딩함\n",
    "    * Data File\n",
    "    * DataFrame, Numpy (전처리)\n",
    "    * Tensor\n",
    "    * Dataset (피쳐+타켓)\n",
    "    * Dataloader (데이터 로드 및 배치 처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 & 데이터로더 살펴보기\n",
    "- Pytorch 에서 배치크기만 데이터를 조절하기 위한 메커니즘\n",
    "- Dataset : 사용 데이터를 기반으로 사용자 정의 클래스 작성\n",
    "- Dataloader : 지정된 Dataset 에서 지정된 batch size 만큼 피쳐와 타켓을 추출하여 전달\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 모듈 로딩 및 데이터 준비\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모듈 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[10,20,30], [20,30,40], [40,50,60], [50,60,70] , [70,80,90]]\n",
    "x_data = torch.IntTensor(array, device=DEVICE)\n",
    "y_data = torch.FloatTensor([[np.mean(x)] for x in array], device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), 2, torch.Size([5, 1]), 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape, x_data.ndim, y_data.shape, y_data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 데이터셋 생성\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### (2-1) TensorDataset 활용 : Dataset의 sub_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset 클래스 로딩\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x764ec819cb20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TensorDataset(x_data, y_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([10, 20, 30], dtype=torch.int32), tensor([20.])),\n",
       " (tensor([10, 20, 30], dtype=torch.int32), tensor([20.])),\n",
       " 5,\n",
       " (tensor([[10, 20, 30],\n",
       "          [20, 30, 40],\n",
       "          [40, 50, 60],\n",
       "          [50, 60, 70],\n",
       "          [70, 80, 90]], dtype=torch.int32),\n",
       "  tensor([[20.],\n",
       "          [30.],\n",
       "          [50.],\n",
       "          [60.],\n",
       "          [80.]])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## __getitem()__()가 호출됨\n",
    "\n",
    "dataset[0], dataset.__getitem__(0), dataset.__len__(), dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2-2] 사용자 정의 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터준비\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=False)\n",
    "data = load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약 타켓 이름을 숫자로 바꾸고 싶은 경우 LabelEncoder, cat.codes 등을 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['frame']\n",
    "featuredf = df[df.columns[0:-1]]\n",
    "targetdf = df[df.columns[-1]].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = torch.FloatTensor(X, device=DEVICE)\n",
    "yt = torch.IntTensor(y, device=DEVICE)\n",
    "# Xt, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 사용자정의 DataSet 클래스\n",
    "# 데이터의 Tensor 변환\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    # 초기화 함수\n",
    "    def __init__(self, X, y):\n",
    "        # 부모 객체로 초기화\n",
    "        super().__init__()\n",
    "\n",
    "        # x, y 데이터 ==> ndarray\n",
    "        if isinstance(X, pd.DataFrame) and isinstance(y, pd.Series):\n",
    "            self.X = X.values\n",
    "            self.y = y.values\n",
    "        else:\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        # ndarray ===> tensor화\n",
    "        self.X = torch.tensor(X, device=DEVICE, dtype=torch.float).to(DEVICE)\n",
    "        self.y = torch.tensor(y, device=DEVICE, dtype=torch.long).reshape(-1,).to(DEVICE)\n",
    "\n",
    "\n",
    "    # 데이터셋의 갯수 체크 함수\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    # 특정 인덱스 데이터 + 라벨 반환 콜백함수 (callback function)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = CustomDataset(X, targetdf)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-2) 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2-3] 학습용, 검증용, 테스트용 Dataset 분할\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 15, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### => PyTorch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 학습용, 검증용, 테스트 데이터 비율 = 7 : 1 : 2\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# 비율을 제공하여 3개 데이터로 나눌 수 있다.\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size], seed)\n",
    "\n",
    "\n",
    "train_set.__len__(), val_set.__len__(), test_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 원래 인덱스 => \n",
      "[42, 95, 30, 64, 52, 35, 130, 40, 82, 17, 108, 94, 68, 97, 117, 127, 41, 44, 57, 140, 149, 32, 23, 102, 16, 113, 71, 18, 67, 66, 0, 25, 101, 112, 91, 3, 59, 116, 86, 84, 106, 142, 43, 39, 26, 98, 93, 20, 87, 19, 120, 114, 7, 63, 76, 89, 36, 45, 37, 56, 58, 122, 51, 145, 24, 21, 105, 62, 15, 11, 48, 133, 88, 50, 6, 134, 111, 8, 49, 75, 69, 124, 4, 147, 80, 100, 99, 141, 47, 107, 13, 109, 129, 28, 38, 53, 121, 5, 55, 31, 73, 74, 54, 29, 12]\n",
      "[22, 104, 81, 1, 103, 125, 85, 2, 96, 128, 27, 118, 77, 110, 146]\n",
      "[72, 139, 131, 60, 65, 92, 135, 83, 14, 34, 137, 10, 119, 9, 148, 79, 78, 70, 144, 143, 123, 115, 61, 132, 90, 46, 126, 136, 33, 138]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x) for x in [ '데이터의 원래 인덱스 => ',train_set.indices, val_set.indices, test_set.indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0756)\n",
      "tensor(0.4418)\n",
      "tensor(0.5369)\n",
      "tensor(0.2991)\n",
      "tensor(0.2700)\n",
      "tensor(0.5641)\n",
      "tensor(0.4180)\n",
      "tensor(0.9769)\n",
      "tensor(0.4710)\n",
      "tensor(0.2614)\n"
     ]
    }
   ],
   "source": [
    "for y in (print(x) for x in [x for x in torch.rand(10)]): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) DataLoader 생성\n",
    "\n",
    "- 학습용\n",
    "- 검증용\n",
    "- 테스트용\n",
    "\n",
    "위 세가지 종류의 데이터로 나누어야함\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 10\n",
    "trainDL = DataLoader(train_set, batch_size=SIZE, shuffle=False, drop_last=False)\n",
    "valDL = DataLoader(val_set, batch_size=SIZE, shuffle=True, drop_last=False) # drop last를 통해 epoch 개수보다 모자른 데이터 학습을 제외시킬 수 있음\n",
    "testDL = DataLoader(test_set, batch_size=SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째\n",
      "tensor([[4.4000, 3.2000, 1.3000, 0.2000, 0.0000],\n",
      "        [5.7000, 3.0000, 4.2000, 1.2000, 1.0000],\n",
      "        [4.8000, 3.1000, 1.6000, 0.2000, 0.0000],\n",
      "        [5.6000, 2.9000, 3.6000, 1.3000, 1.0000],\n",
      "        [6.9000, 3.1000, 4.9000, 1.5000, 1.0000],\n",
      "        [5.0000, 3.2000, 1.2000, 0.2000, 0.0000],\n",
      "        [7.4000, 2.8000, 6.1000, 1.9000, 2.0000],\n",
      "        [5.0000, 3.5000, 1.3000, 0.3000, 0.0000],\n",
      "        [5.8000, 2.7000, 3.9000, 1.2000, 1.0000],\n",
      "        [5.1000, 3.5000, 1.4000, 0.3000, 0.0000]])\n",
      "2번째\n",
      "tensor([[6.7000, 2.5000, 5.8000, 1.8000, 2.0000],\n",
      "        [5.6000, 2.7000, 4.2000, 1.3000, 1.0000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000, 1.0000],\n",
      "        [6.2000, 2.9000, 4.3000, 1.3000, 1.0000],\n",
      "        [7.7000, 3.8000, 6.7000, 2.2000, 2.0000],\n",
      "        [6.1000, 3.0000, 4.9000, 1.8000, 2.0000],\n",
      "        [4.5000, 2.3000, 1.3000, 0.3000, 0.0000],\n",
      "        [5.1000, 3.8000, 1.9000, 0.4000, 0.0000],\n",
      "        [4.9000, 2.4000, 3.3000, 1.0000, 1.0000],\n",
      "        [6.7000, 3.1000, 5.6000, 2.4000, 2.0000]])\n",
      "3번째\n",
      "tensor([[5.9000, 3.0000, 5.1000, 1.8000, 2.0000],\n",
      "        [5.2000, 4.1000, 1.5000, 0.1000, 0.0000],\n",
      "        [5.1000, 3.3000, 1.7000, 0.5000, 0.0000],\n",
      "        [7.1000, 3.0000, 5.9000, 2.1000, 2.0000],\n",
      "        [5.4000, 3.9000, 1.3000, 0.4000, 0.0000],\n",
      "        [5.7000, 2.5000, 5.0000, 2.0000, 2.0000],\n",
      "        [6.1000, 2.8000, 4.0000, 1.3000, 1.0000],\n",
      "        [5.7000, 3.8000, 1.7000, 0.3000, 0.0000],\n",
      "        [5.8000, 2.7000, 4.1000, 1.0000, 1.0000],\n",
      "        [5.6000, 3.0000, 4.5000, 1.5000, 1.0000]])\n",
      "4번째\n",
      "tensor([[5.1000, 3.5000, 1.4000, 0.2000, 0.0000],\n",
      "        [5.0000, 3.0000, 1.6000, 0.2000, 0.0000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000, 2.0000],\n",
      "        [6.8000, 3.0000, 5.5000, 2.1000, 2.0000],\n",
      "        [6.1000, 3.0000, 4.6000, 1.4000, 1.0000],\n",
      "        [4.6000, 3.1000, 1.5000, 0.2000, 0.0000],\n",
      "        [5.2000, 2.7000, 3.9000, 1.4000, 1.0000],\n",
      "        [6.5000, 3.0000, 5.5000, 1.8000, 2.0000],\n",
      "        [6.7000, 3.1000, 4.7000, 1.5000, 1.0000],\n",
      "        [5.4000, 3.0000, 4.5000, 1.5000, 1.0000]])\n",
      "5번째\n",
      "tensor([[4.9000, 2.5000, 4.5000, 1.7000, 2.0000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000, 2.0000],\n",
      "        [5.0000, 3.5000, 1.6000, 0.6000, 0.0000],\n",
      "        [5.1000, 3.4000, 1.5000, 0.2000, 0.0000],\n",
      "        [5.0000, 3.4000, 1.6000, 0.4000, 0.0000],\n",
      "        [5.1000, 2.5000, 3.0000, 1.1000, 1.0000],\n",
      "        [5.0000, 2.3000, 3.3000, 1.0000, 1.0000],\n",
      "        [5.4000, 3.4000, 1.7000, 0.2000, 0.0000],\n",
      "        [6.3000, 2.3000, 4.4000, 1.3000, 1.0000],\n",
      "        [5.1000, 3.8000, 1.5000, 0.3000, 0.0000]])\n",
      "6번째\n",
      "tensor([[6.9000, 3.2000, 5.7000, 2.3000, 2.0000],\n",
      "        [5.8000, 2.8000, 5.1000, 2.4000, 2.0000],\n",
      "        [5.0000, 3.4000, 1.5000, 0.2000, 0.0000],\n",
      "        [6.1000, 2.9000, 4.7000, 1.4000, 1.0000],\n",
      "        [6.8000, 2.8000, 4.8000, 1.4000, 1.0000],\n",
      "        [5.5000, 2.5000, 4.0000, 1.3000, 1.0000],\n",
      "        [5.5000, 3.5000, 1.3000, 0.2000, 0.0000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.3000, 0.0000],\n",
      "        [4.9000, 3.6000, 1.4000, 0.1000, 0.0000],\n",
      "        [6.3000, 3.3000, 4.7000, 1.6000, 1.0000]])\n",
      "7번째\n",
      "tensor([[6.6000, 2.9000, 4.6000, 1.3000, 1.0000],\n",
      "        [7.7000, 2.8000, 6.7000, 2.0000, 2.0000],\n",
      "        [6.4000, 3.2000, 4.5000, 1.5000, 1.0000],\n",
      "        [6.7000, 3.0000, 5.2000, 2.3000, 2.0000],\n",
      "        [4.8000, 3.4000, 1.9000, 0.2000, 0.0000],\n",
      "        [5.1000, 3.7000, 1.5000, 0.4000, 0.0000],\n",
      "        [7.6000, 3.0000, 6.6000, 2.1000, 2.0000],\n",
      "        [6.0000, 2.2000, 4.0000, 1.0000, 1.0000],\n",
      "        [5.7000, 4.4000, 1.5000, 0.4000, 0.0000],\n",
      "        [4.8000, 3.4000, 1.6000, 0.2000, 0.0000]])\n",
      "8번째\n",
      "tensor([[5.3000, 3.7000, 1.5000, 0.2000, 0.0000],\n",
      "        [6.3000, 2.8000, 5.1000, 1.5000, 2.0000],\n",
      "        [5.6000, 3.0000, 4.1000, 1.3000, 1.0000],\n",
      "        [7.0000, 3.2000, 4.7000, 1.4000, 1.0000],\n",
      "        [4.6000, 3.4000, 1.4000, 0.3000, 0.0000],\n",
      "        [6.1000, 2.6000, 5.6000, 1.4000, 2.0000],\n",
      "        [6.4000, 2.7000, 5.3000, 1.9000, 2.0000],\n",
      "        [4.4000, 2.9000, 1.4000, 0.2000, 0.0000],\n",
      "        [5.0000, 3.3000, 1.4000, 0.2000, 0.0000],\n",
      "        [6.6000, 3.0000, 4.4000, 1.4000, 1.0000]])\n",
      "9번째\n",
      "tensor([[5.6000, 2.5000, 3.9000, 1.1000, 1.0000],\n",
      "        [6.7000, 3.3000, 5.7000, 2.1000, 2.0000],\n",
      "        [5.0000, 3.6000, 1.4000, 0.2000, 0.0000],\n",
      "        [6.5000, 3.0000, 5.2000, 2.0000, 2.0000],\n",
      "        [5.5000, 2.4000, 3.8000, 1.1000, 1.0000],\n",
      "        [6.3000, 3.3000, 6.0000, 2.5000, 2.0000],\n",
      "        [5.7000, 2.8000, 4.1000, 1.3000, 1.0000],\n",
      "        [6.9000, 3.1000, 5.1000, 2.3000, 2.0000],\n",
      "        [4.6000, 3.2000, 1.4000, 0.2000, 0.0000],\n",
      "        [7.3000, 2.9000, 6.3000, 1.8000, 2.0000]])\n",
      "10번째\n",
      "tensor([[4.3000, 3.0000, 1.1000, 0.1000, 0.0000],\n",
      "        [7.2000, 3.6000, 6.1000, 2.5000, 2.0000],\n",
      "        [7.2000, 3.0000, 5.8000, 1.6000, 2.0000],\n",
      "        [5.2000, 3.4000, 1.4000, 0.2000, 0.0000],\n",
      "        [4.4000, 3.0000, 1.3000, 0.2000, 0.0000],\n",
      "        [5.5000, 2.3000, 4.0000, 1.3000, 1.0000],\n",
      "        [5.6000, 2.8000, 4.9000, 2.0000, 2.0000],\n",
      "        [5.4000, 3.9000, 1.7000, 0.4000, 0.0000],\n",
      "        [5.7000, 2.8000, 4.5000, 1.3000, 1.0000],\n",
      "        [5.4000, 3.4000, 1.5000, 0.4000, 0.0000]])\n",
      "11번째\n",
      "tensor([[6.1000, 2.8000, 4.7000, 1.2000, 1.0000],\n",
      "        [6.4000, 2.9000, 4.3000, 1.3000, 1.0000],\n",
      "        [6.5000, 2.8000, 4.6000, 1.5000, 1.0000],\n",
      "        [4.7000, 3.2000, 1.6000, 0.2000, 0.0000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.1000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# epoch당 반복단위 ==> iterator\n",
    "# DataLoader는 iterating 시 epoch size 행만큼의 x, y가 나옴\n",
    "for _, (x, y) in enumerate(trainDL):\n",
    "    ## 로더에서 가지고온 데이터 만큼 학습 진행\n",
    "    print(f'{_ + 1}번째')\n",
    "    print(torch.concat([x, y.reshape(-1,1)], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 모델 클래스 정의\n",
    "<hr>\n",
    "\n",
    "- 입/출력 피쳐수\n",
    "- 층 수 \n",
    "- 타켓 출력\n",
    "- 은닉층의 노드수\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "- 구조 설계\n",
    "    * 입력층 : 입력 <= 피쳐 갯수, iris 4개\n",
    "    * 은닉층 : 사용자가 임의로 정의\n",
    "    * 출력층 : 출력 <= [분류] 타켓 클래스 갯수 [회귀] 1개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential()\n",
    "        self.seq.add_module('fc1', nn.Linear(input_size, hidden_size, device=DEVICE))\n",
    "        self.seq.add_module('fc1_act', nn.ReLU())\n",
    "        self.seq.add_module('fc2', nn.Linear(hidden_size, num_classes, device=DEVICE))\n",
    "\n",
    "        # self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.fc1_act = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        # self.fc2_act = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.fc1_act(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.fc2_act(x)\n",
    "        x = self.seq(x)\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 학습 준비 : 실행디바이스, 모델, 최적화, 손실함수, 학습횟수, 학습함수, 평가함수, 예측함수\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 디바이스 설정\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 학습횟수\n",
    "EPOCHS = 10000\n",
    "\n",
    "# 모델 인스턴스\n",
    "flower = CustomModel(featuredf.shape[1], 12, np.unique(targetdf).size)\n",
    "\n",
    "# 손실함수 \n",
    "LOSS_FN = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([5]), torch.Size([5, 3]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (seq): Sequential(\n",
       "    (fc1): Linear(in_features=4, out_features=12, bias=True)\n",
       "    (fc1_act): ReLU()\n",
       "    (fc2): Linear(in_features=12, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "metric = torchmetrics.classification.MulticlassAccuracy(num_classes=3, average='micro')\n",
    "metric_train = torchmetrics.classification.MulticlassAccuracy(num_classes=3, average='micro')\n",
    "\n",
    "def training(model, loss_fn, optimizer, train_dl, epochs=EPOCHS):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=5, eps=1e-8)\n",
    "\n",
    "    tmptrainacc = []\n",
    "    for _, (x, y) in enumerate(train_dl):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        # y = nn.functional.one_hot(y, 3).float()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        acc = metric(y_pred, y)\n",
    "        tmptrainacc.append(acc)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    tmpacc = []\n",
    "    for _, (x_test, y_test) in enumerate(testDL):\n",
    "        x_test = x_test.to(DEVICE)\n",
    "        y_test = y_test.to(DEVICE)\n",
    "        # y_test = nn.functional.one_hot(y_test, 3).float()\n",
    "\n",
    "        y_test_predict = model(x_test)\n",
    "        acc = metric(y_test_predict, y_test)\n",
    "        tmpacc.append(acc.item())\n",
    "    y_test_predict = model(x_test.to(DEVICE))\n",
    "    print(f\"{y.shape, y_pred.shape}\")\n",
    "    \n",
    "    # print(f'Epoch: {epoch+1}, Loss: {loss.item()}, Accuracy : train = {np.mean(tmptrainacc)} test = {np.mean(tmpacc)}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "training(flower, LOSS_FN, torch.optim.Adam, trainDL, 10)\n",
    "\n",
    "flower.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 검증 및 평가 진행함수\n",
    "# 매개변수 dataLoader : 검증 또는 테스트 데이터셋에 대한 Loader\n",
    "# \n",
    "\n",
    "def testing(model, loss_fn, dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        tmptrainacc = []\n",
    "        for _, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            # y = nn.functional.one_hot(y, 3).float()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            tmptrainacc.append(loss)\n",
    "\n",
    "    return np.mean(tmptrainacc)      \n",
    "        # tmpacc = []\n",
    "        # for _, (x_test, y_test) in enumerate(dataloader):\n",
    "        #     x_test = x_test.to(DEVICE)\n",
    "        #     y_test = y_test.to(DEVICE)\n",
    "        #     # y_test = nn.functional.one_hot(y_test, 3).float()\n",
    "\n",
    "        #     y_test_predict = model(x_test)\n",
    "\n",
    "        #     tmpacc.append(acc.item())\n",
    "        # y_test_predict = model(x_test.to(DEVICE))\n",
    "        # print(f\"{y.shape, y_pred.shape}\")\n",
    "        \n",
    "        # print(f'Epoch: {epoch+1}, Loss: {loss.item()}, Accuracy : train = {np.mean(tmptrainacc)} test = {np.mean(tmpacc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 1, Loss : train = 0.7861799001693726 test = 0.8946115970611572\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 2, Loss : train = 0.7295676469802856 test = 0.6560657024383545\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 3, Loss : train = 0.5609229803085327 test = 0.5008491277694702\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 4, Loss : train = 0.46419721841812134 test = 0.4513162672519684\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 5, Loss : train = 0.3899132311344147 test = 0.357729434967041\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 6, Loss : train = 0.3366197943687439 test = 0.31142929196357727\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 7, Loss : train = 0.293018639087677 test = 0.23709315061569214\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 8, Loss : train = 0.2463933229446411 test = 0.20581971108913422\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 9, Loss : train = 0.20545415580272675 test = 0.20599254965782166\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 10, Loss : train = 0.17739221453666687 test = 0.19359919428825378\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 11, Loss : train = 0.1553117334842682 test = 0.17565137147903442\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 12, Loss : train = 0.13712680339813232 test = 0.1405177116394043\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 13, Loss : train = 0.1249120682477951 test = 0.11460596323013306\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 14, Loss : train = 0.1176091879606247 test = 0.14178961515426636\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 15, Loss : train = 0.11538910865783691 test = 0.10234372317790985\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 16, Loss : train = 0.11839139461517334 test = 0.08545839786529541\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 17, Loss : train = 0.12793894112110138 test = 0.1425885260105133\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 18, Loss : train = 0.12040271610021591 test = 0.0975794717669487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workplace/miniconda3/envs/EXAM_DL2/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/workplace/miniconda3/envs/EXAM_DL2/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 19, Loss : train = 0.1123715415596962 test = 0.08442850410938263\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 20, Loss : train = 0.09909794479608536 test = 0.08671922236680984\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 21, Loss : train = 0.09178783744573593 test = 0.09012433886528015\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 22, Loss : train = 0.08629320561885834 test = 0.09874676167964935\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 23, Loss : train = 0.05255081504583359 test = 0.12854909896850586\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 24, Loss : train = 0.07656274735927582 test = 0.1044299453496933\n",
      "(torch.Size([5]), torch.Size([5, 3]))\n",
      "Epoch: 25, Loss : train = 0.04362393915653229 test = 0.13350088894367218\n"
     ]
    }
   ],
   "source": [
    "## == > 지정된 횟수 만큼 처음부터 끝까지 학습 및 검증 진행\n",
    "## == > 목표 : 최적(Error 최소화)의 W, b를 가진 모델 완성\n",
    "## == > \n",
    "\n",
    "valList = []\n",
    "for eps in range(EPOCHS):\n",
    "    train_loss = training(flower, LOSS_FN, torch.optim.Adam, trainDL, 1)\n",
    "    val_loss = testing(flower, LOSS_FN, valDL)\n",
    "\n",
    "    print(f'Epoch: {eps+1}, Loss : train = {train_loss} test = {val_loss}')\n",
    "    # 조기 종료 기준 ==> 조건 : val_loss가 지정된 횟수 (예: 5) 이상 개선이 안되면 학습 종료\n",
    "    valList.append(val_loss)\n",
    "    if np.mean(valList[-5:]) > np.mean(valList[-10:-5]):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
